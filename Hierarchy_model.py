#!/usr/bin/env python
#coding:utf-8
"""
  Author:   weiyiliu --<weiyiliu@us.ibm.com>
  Purpose:  Hierarchy model (based on Condition Model & DCGAN)
  Created:  04/13/17
"""
import numpy as np
import tensorflow as tf
import time
import os
import pickle
import glob
import math

from utils import *
import Conditional_model as C_model
from Conditional_Topology_MAIN import graph2Adj, generate_graph

debugFlag = True

class Hierarchy_adjMatrix_Generator(object):
    """
    @purpose: éšä¾¿ç»™å®šä¸€ä¸ªç½‘ç»œï¼Œç”Ÿæˆå…·æœ‰å¯¹åº”ç‰¹æ€§çš„ç½‘ç»œ
    """
    def __init__(self,
                sess, dataset_name, permutation_num,
                epoch=10, learning_rate=0.0002, Momentum=0.5,
                batch_size=10,
                generatorFilter=50, discriminatorFilter=50,
                generatorFC=1024, discriminatorFC=1024,
                training_info_dir="facebook_partition_info.pickle",
                OutDegree_Length=1,
                inputPartitionDIR="facebook", checkpointDIR="condition_checkpoint", sampleDIR="condition_samples",reconstructDIR="reconstruction",
                link_possibility=0.5,
                trainedFlag=False
            ):
        """
        @purpose
            set all hyperparameters
        @inputs:
            sess:               Current Tensorflow Session
            dataset_name:       Current Dataset Name
            permutation_step:   Permutate Num
            epoch:              Epochs Number for Whole Datsets [20]
            learning_rate:      Init Learning Rate for Adam [0.0002]
            Momentum:           é‡‡ç”¨ADAMç®—æ³•æ—¶æ‰€éœ€è¦çš„Momentumçš„å€¼ [0.5] --- based on DCGAN
            batch_size:         æ¯ä¸€æ‰¹è¯»å–çš„adj-matå¤§å° [10]
            generatorFilter:    ç”Ÿæˆå™¨ åˆå§‹çš„filteræ•°å€¼ [50] --- æ³¨: ç®—æ³•ä¸­æ¯ä¸€å±‚çš„filteréƒ½è®¾ç½®ä¸ºè¯¥å€¼çš„2å€ based on DCGAN
            discriminatorFilter:åˆ¤åˆ«å™¨ åˆå§‹çš„filteræ•°å€¼ [50] --- æ³¨: ç®—æ³•ä¸­æ¯ä¸€å±‚çš„filteréƒ½è®¾ç½®ä¸ºè¯¥å€¼çš„2å€ based on DCGAN
            generatorFC:        ç”Ÿæˆå™¨çš„å…¨è¿æ¥å±‚çš„ç¥ç»å…ƒä¸ªæ•° [1024]
            discriminatorFC:    åˆ¤åˆ«å™¨çš„å…¨è¿æ¥å±‚çš„ç¥ç»å…ƒä¸ªæ•° [1024]
            training_info_dir:  [trainable_data_size, inputMatSize] æ‰€åœ¨ä½ç½®
            OutDegree_Length:   å½“å‰ AdjMatrixçš„ å‡ºåº¦å‘é‡é•¿åº¦ï¼Œè¡¨ç¤ºçš„æ˜¯ç±»åˆ« [28]  --- ç›¸å½“äºDCGANä¸­çš„ y_lim [æ‰‹å†™æ•°å­—ä¸­çš„ç±»åˆ«ä¿¡æ¯~]
            inputPartitionDIR:  åˆ†å‰²åçš„çŸ©é˜µçš„å­˜æ¡£ç‚¹ [facebook] --- æ³¨: æœ€å¥½ä¸ dataset_name ä¿æŒä¸€è‡´ï¼Œåªä¸è¿‡è¿™é‡ŒæŒ‡çš„æ˜¯å½“å‰dataset_nameæ‰€åœ¨çš„folder
            checkpointDIR:      å­˜æ¡£ç‚¹ åœ°å€ [condition_checkpoint]
            sampleDIR:          é‡‡æ ·å¾—åˆ°çš„ç½‘ç»œ è¾“å‡ºåœ°å€ [condition_samples]
            reconstructDIR:     é‡æ„ç½‘ç»œçš„å­˜æ”¾åœ°å€
            link_possibility:   é‡æ„ç½‘ç»œæ—¶æŒ‡å®šçš„ è¿æ¥æƒé‡
            trainedFlag:        æ˜¯å¦éœ€è¦å¯¹æ¯ä¸€å±‚è¿›è¡Œè®­ç»ƒ. å½“ä¸ºFALSEè¡¨ç¤ºéœ€è¦è®­ç»ƒï¼ŒTRUEè¡¨ç¤ºä¸éœ€è¦è®­ç»ƒ [False]
        """
        # GAN å‚æ•°åˆå§‹åŒ–
        self.sess                = sess
        self.dataset_name        = dataset_name
        self.permutation_num     = permutation_num
        self.epoch               = epoch
        self.learning_rate       = learning_rate
        self.Momentum            = Momentum
        self.batch_size          = batch_size
        self.generatorFilter     = generatorFilter
        self.discriminatorFilter = discriminatorFilter
        self.generatorFC         = generatorFC
        self.discriminatorFC     = discriminatorFC
        # input / output size åˆå§‹åŒ–
        train_list = pickle.load(open(training_info_dir,'rb'))
        self.trainable_data_size_list = [info[0] for info in train_list]
        self.inputMat_H_list          = [info[1] for info in train_list]
        self.inputMat_W_list          = [info[1] for info in train_list]
        self.outputMat_H_list         = [info[1] for info in train_list]
        self.outputMat_W_list         = [info[1] for info in train_list]
        self.OutDegree_Length         = OutDegree_Length
        # ç”¨ä½œGeneratorçš„è¾“å…¥ï¼Œç”Ÿæˆ å½“å‰ç½‘ç»œ
        self.InitSampleLength_list    = [info[1] for info in train_list]
        # æŒ‡å®š è·¯å¾„
        self.inputPartitionDIR   = inputPartitionDIR
        self.checkpointDIR       = checkpointDIR
        self.sampleDIR           = sampleDIR
        self.reconstructDIR      = reconstructDIR
        # ç”¨ä½œç”Ÿæˆ æ‹“æ‰‘ç»“æ„ çš„æ–¹å¼
        self.link_possibility    = link_possibility

        # æ„å»º GAN~
        if trainedFlag is False:
            self.per_layer_modelConstrunction()
            show_all_variables() # TFä¸­çš„æ‰€æœ‰å˜é‡
            print('Trained Layers Process DOWN...')

    def per_layer_modelConstrunction(self):
        print('\n============================================================================')
        print('Model Construction ...')
        print('============================================================================')

        self.reconstructNet_per_layer = []
        for layer_idx in range(len(self.trainable_data_size_list)):
            """è®­ç»ƒæ¯ä¸€ä¸ªæ¨¡å‹~"""
            model_name = "%s_Mat_%d_Trainable_%d"%(self.dataset_name, self.inputMat_H_list[layer_idx], self.trainable_data_size_list[layer_idx])
            if debugFlag is True:
                print('current model: ', model_name)
            # with tf.device('cpu:0'):
            with tf.Session() as sess:
                model = C_model.Condition_adjMatrix_Generator(
                    sess,dataset_name = self.dataset_name,
                    epoch=self.epoch,learning_rate=self.learning_rate,Momentum=self.Momentum,
                    batch_size=self.batch_size,
                    generatorFilter=self.generatorFilter,discriminatorFilter=self.discriminatorFilter,
                    generatorFC=self.generatorFC,discriminatorFC=self.discriminatorFC,
                    trainable_data_size=self.trainable_data_size_list[layer_idx],
                    inputMat_H=self.inputMat_H_list[layer_idx],
                    inputMat_W=self.inputMat_W_list[layer_idx],
                    outputMat_H=self.outputMat_H_list[layer_idx],
                    outputMat_W=self.outputMat_W_list[layer_idx],
                    OutDegree_Length=self.OutDegree_Length,
                    InitGen_Length=self.InitSampleLength_list[layer_idx],
                    inputPartitionDIR=self.inputPartitionDIR,
                    checkpointDIR=os.path.join(self.checkpointDIR, model_name),
                    sampleDIR=os.path.join(self.sampleDIR, model_name),
                    link_possibility=self.link_possibility
                )
                model.train()
                model.saveModel()
                re_Net = model.reconstructMat(type="Hierarchy")

                self.reconstructNet_per_layer.append(re_Net)

    def modelConstruction(self):
        # step.0 ç”ŸæˆHierarchy GAN çš„Adj ä»¥åŠ åŸå§‹æ•°æ®çš„ GAN
        if not os.path.exists('%s_adjs.pickle'%self.dataset_name):
            # 1. è¯»å–trained åçš„æ¯ä¸€å±‚çš„æ•°æ®, å¹¶ç”Ÿæˆ ä¿å­˜äºtrained_graph_listä¸­
            trained_layer_path = os.path.join(self.reconstructDIR,self.dataset_name,"Hierarchy",'')
            trained_graph_adj_list = []
            paths = glob.glob(trained_layer_path+"%s_*.nxgraph"%self.dataset_name)
            if debugFlag is True:
                print('all trained layer paths: ', paths)
            for path in paths:
                graph = pickle.load(open(path,'rb'))
                if debugFlag is True:
                    print('trained graph size: ',len(graph.nodes()))
                adj = graph2Adj(graph, max_size = -1)
                if debugFlag is True:
                    print('current adj shape: ', adj.shape)
                trained_graph_adj_list.append(adj)

            # 2. è¯»å–åŸå§‹ç½‘ç»œï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„adj
            original_graph_path = os.path.join("data", self.dataset_name, '')
            origin_graph = generate_graph(original_graph_path,self.dataset_name,-1)
            if debugFlag is True:
                print('original graph size: ',len(origin_graph.nodes()))
            origin_adj = graph2Adj(origin_graph,max_size=-1)
            if debugFlag is True:
                print('original adj shape: ', origin_adj.shape)

            pickle.dump([trained_graph_adj_list,origin_adj],open('%s_adjs.pickle'%self.dataset_name,'wb'))
        else:
            [trained_graph_adj_list,origin_adj] = pickle.load(open('%s_adjs.pickle'%self.dataset_name,'rb'))
            if debugFlag is True:
                for i in trained_graph_adj_list:
                    print('trained graph ajd shape :', i.shape)
                print('original adj shape: ', origin_adj.shape)

        """ add 2017.04.18 """
        # ==============================================
        # 1. å°†trained_graph_list æ”¹æˆä¹Ÿéœ€è¦è€ƒè™‘åŸç½‘ç»œçš„æƒ…å†µ
        # ==============================================
        trained_graph_adj_list.append(origin_adj)

        # ==============================================
        # 2. Permute Adj to generate more adjs
        # ==============================================
        permute_number = self.permutation_num
        if debugFlag is True:
            print("permuting each adj for %d times"%permute_number)
        trained_graph_adj_list = permute_adjs(trained_graph_adj_list,permute_number)
        if debugFlag is True:
            print('permuted adjs number: ', len(trained_graph_adj_list))
        """ end add """

        # step.1 åˆ›å»ºWeight~
        self.trained_graph_weight_list = []
        self.layer_weight_list = [] # for Hierarchy GAN~ ğŸ˜€
        count = 0
        for adj in trained_graph_adj_list:
            """æ¯ä¸€ä¸ªé‚»æ¥çŸ©é˜µ ç”Ÿæˆä¸ä¸€æ ·çš„æƒé‡"""
            layer_weight = tf.Variable(tf.random_uniform([1],minval=0,maxval=1),name="weight_%d"%count)
            self.layer_weight_list.append(layer_weight)
            adj_layer_weight = layer_weight*tf.ones(shape=adj.shape) # æ‰©å±•åˆ°æ¯ä¸€ä¸ªç»´åº¦ä¸Š~
            self.trained_graph_weight_list.append(adj_layer_weight)

            count += 1

        # åˆ›å»ºbias
        self.bias = tf.Variable(tf.random_uniform([1],minval=0,maxval=1),name="bias")

        # step.2 logit
        tmp = [self.trained_graph_weight_list[idx]*trained_graph_adj_list[idx] for idx in range(len(self.trained_graph_weight_list))]
        self.logits = tf.add(tf.add_n(tmp,name="Layered_results"),self.bias)

        # step.3 loss
        origin_adj = tf.to_float(origin_adj)
        """ç®—L1è·ç¦»"""
        # self.loss = tf.reduce_mean(tf.square(origin_adj-self.logits)) # L2-norm
        # self.loss = tf.reduce_mean(tf.abs(origin_adj-self.logits))      # L1-norm
        """ç®—åº¦ä¹‹é—´çš„å·®åˆ«"""
        # zeros = tf.zeros(shape=origin_adj.shape)
        # self.loss = tf.reduce_sum(tf.to_float(tf.where(tf.not_equal(origin_adj-self.logits,zeros))))
        """å°è¯•ç®—ä¸¤ä¸ªåº¦åˆ†å¸ƒä¹‹é—´çš„KLè·ç¦»"""
        # self.loss = tf.contrib.distributions.kl(tf.reduce_sum(origin_adj,1), tf.reduce_sum(adj,1))
        self.logits = self.logits + 0.000001 * tf.ones(shape=origin_adj.shape) # ä¿è¯åˆ†æ¯ä¸ä¸º0
        y = origin_adj/self.logits

        self.loss = tf.abs(tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y)))

        """learning rate decay... from TF_API"""
        start_learning_rate = 0.1
        global_step = tf.Variable(0, trainable=False)
        decay_step = 1000
        decay_rate = 0.96
        learning_rate = tf.train.exponential_decay(learning_rate=start_learning_rate,
                                                    global_step=global_step,
                                                    decay_steps=decay_step,
                                                    decay_rate=decay_rate)

        # step.4 optimizer
        # self.opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)
        self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)

    def train(self, training_step=10000):
        """
        @input training_step è®­ç»ƒæ‰€éœ€æ­¥éª¤
        @return weight åˆ—è¡¨ï¼Œ reconstructed adj
        """
        tf.global_variables_initializer().run()

        for step in range(training_step):
            self.sess.run(self.opt)
            info = ''
            for idx in range(len(self.layer_weight_list)):
                tmp = 'W_%d: [%.4f] | '%(idx, self.sess.run(self.layer_weight_list[idx]))
                info += tmp
            info += " bias: [%.4f]"%self.sess.run(self.bias)
            if len(self.layer_weight_list) <= 4:
                print('step: [%d]/[%d], loss value: %.4f'%(step+1, training_step, self.sess.run(self.loss)), info)
            else:
                print('step: [%d]/[%d], loss value: %.4f'%(step+1, training_step, self.sess.run(self.loss)))

            if self.sess.run(self.loss) <= 0.01:
                break

        weight_list = [self.sess.run(self.layer_weight_list[idx]) for idx in range(len(self.layer_weight_list))]
        tmp_re_adj_raw = self.sess.run(self.logits)
        # meanValue = tf.reduce_mean(tmp_re_adj_raw)
        # tmp_re_adj_raw = tmp_re_adj_raw - meanValue*tf.ones(shape = tmp_re_adj_raw.shape)
        # å…ˆå°† åŸçŸ©é˜µ æ­£åˆ™åŒ–ï¼Œä½¿ä¹‹æŠ•å½±åˆ° [0, 1]ç©ºé—´ä¸­
        maxValue = tf.reduce_max(tmp_re_adj_raw)
        tmp_re_adj_raw_norm = tmp_re_adj_raw/maxValue
        # å†å¹³ç§»å‡å€¼äº-meanValueå¤„ï¼Œè¿™æ˜¯ä¸ºäº†å¥‘åˆ sigmoidå‡½æ•°çš„å›¾åƒç‰¹å¾.
        """
        å› ä¸º sigmoid(Xâ‰¥0) â‰¥ 0.5, è€Œtmp_re_adj_raw_normä¸­çš„å€¼å‡å¤§äº0,
        æ‰€ä»¥ å¦‚æœä¸è¿›è¡Œå¹³ç§»ï¼Œä¼šå¯¼è‡´é€šè¿‡sigmoidæ˜ å°„ä¹‹åçš„æ‰€æœ‰èŠ‚ç‚¹çš„å€¼éƒ½åœ¨0.5ä»¥ä¸Š
        """
        tmp_re_adj_raw = tmp_re_adj_raw_norm - 0.5*tf.ones(shape = tmp_re_adj_raw.shape)

        reconstructed_Adj = tf.sigmoid(tmp_re_adj_raw)
        return weight_list, reconstructed_Adj.eval()


