#!/usr/bin/env python
#coding:utf-8
"""
  Author:   weiyiliu --<weiyiliu@us.ibm.com>
  Purpose:  Condition model (based on DCGAN)
  Created:  04/08/17
"""
import numpy as np
import tensorflow as tf
import time
import os
import sys
import pickle
import glob
import math
import networkx as nx

from utils import *

debugFlag = False

class Condition_adjMatrix_Generator(object):
    """
    @purpose: éšä¾¿ç»™å®šä¸€ä¸ªç½‘ç»œï¼Œç”Ÿæˆå…·æœ‰å¯¹åº”ç‰¹æ€§çš„ç½‘ç»œ
    """
    def __init__(self,
                sess, dataset_name,
                epoch=10, learning_rate=0.0002, Momentum=0.5,
                batch_size=10,
                generatorFilter=50, discriminatorFilter=50,
                generatorFC=1024, discriminatorFC=1024,
                trainable_data_size=20000,
                inputMat_H=28, inputMat_W=28, outputMat_H=28, outputMat_W=28,
                OutDegree_Length=28, InitGen_Length=28,
                inputPartitionDIR="WS_test", checkpointDIR="checkpoint", sampleDIR="samples", reconstructDIR="reconstruction",
                link_possibility=0.5
            ):
        """
        @purpose
            set all hyperparameters
        @inputs:
            sess:               Current Tensorflow Session
            dataset_name:       Current Dataset Name
            epoch:              Epochs Number for Whole Datsets [20]
            learning_rate:      Init Learning Rate for Adam [0.0002]
            Momentum:           é‡‡ç”¨ADAMç®—æ³•æ—¶æ‰€éœ€è¦çš„Momentumçš„å€¼ [0.5] --- based on DCGAN
            batch_size:         æ¯ä¸€æ‰¹è¯»å–çš„adj-matå¤§å° [10]
            generatorFilter:    ç”Ÿæˆå™¨ åˆå§‹çš„filteræ•°å€¼ [50] --- æ³¨: ç®—æ³•ä¸­æ¯ä¸€å±‚çš„filteréƒ½è®¾ç½®ä¸ºè¯¥å€¼çš„2å€ based on DCGAN
            discriminatorFilter:åˆ¤åˆ«å™¨ åˆå§‹çš„filteræ•°å€¼ [50] --- æ³¨: ç®—æ³•ä¸­æ¯ä¸€å±‚çš„filteréƒ½è®¾ç½®ä¸ºè¯¥å€¼çš„2å€ based on DCGAN
            generatorFC:        ç”Ÿæˆå™¨çš„å…¨è¿æ¥å±‚çš„ç¥ç»å…ƒä¸ªæ•° [1024]
            discriminatorFC:    åˆ¤åˆ«å™¨çš„å…¨è¿æ¥å±‚çš„ç¥ç»å…ƒä¸ªæ•° [1024]
            trainable_data_size:è®­ç»ƒæ•°æ®çš„æ€»ä¸ªæ•° [20000]
            inputMat_H:         è¾“å…¥é‚»æ¥çŸ©é˜µçš„Height [28] --- æ³¨: ä¸ºé¿å…å·ç§¯çš„æ—¶å€™å‡ºç°å°æ•°ï¼Œå› æ­¤è¿™é‡Œå»ºè®®è®¾ç½®æˆ4çš„æ•´æ•°å€ï¼ŒåŸå› åœ¨äºæˆ‘ä»¬éœ€è¦è¿›è¡Œ2æ¬¡å·ç§¯æ“ä½œï¼Œæ¯æ¬¡stride=2ï¼Œæ‰€ä»¥ä¼šæ¯”åŸå›¾åƒç¼©å°4å€
            inputMat_W:         è¾“å…¥é‚»æ¥çŸ©é˜µçš„Width [28] --- æ³¨: åŒä¸Š
            outputMat_H:        è¾“å‡ºé‚»æ¥çŸ©é˜µçš„Height [28] --- æ³¨: åŒä¸Š
            outputMat_W:        è¾“å‡ºé‚»æ¥çŸ©é˜µçš„Width [28] --- æ³¨: åŒä¸Š
            OutDegree_Length:   å½“å‰ AdjMatrixçš„ å‡ºåº¦å‘é‡é•¿åº¦ï¼Œè¡¨ç¤ºçš„æ˜¯ç±»åˆ« [28]  --- ç›¸å½“äºDCGANä¸­çš„ y_lim [æ‰‹å†™æ•°å­—ä¸­çš„ç±»åˆ«ä¿¡æ¯~]
            InitGen_Length:     ç”¨ä½œ GENçš„è¾“å…¥å‘é‡ï¼Œæœ€å¥½ä¸å½“å‰ AdjMatrixçš„ inputMatå¤§å° ä¿æŒä¸€è‡´ [28] --- ç›¸å½“äºDCGANä¸­çš„ z_lim
            inputPartitionDIR:  åˆ†å‰²åçš„çŸ©é˜µçš„å­˜æ¡£ç‚¹ [WS_test] --- æ³¨: æœ€å¥½ä¸ dataset_name ä¿æŒä¸€è‡´ï¼Œåªä¸è¿‡è¿™é‡ŒæŒ‡çš„æ˜¯å½“å‰dataset_nameæ‰€åœ¨çš„folder
            checkpointDIR:      å­˜æ¡£ç‚¹ åœ°å€ [checkpoint]
            sampleDIR:          é‡‡æ ·å¾—åˆ°çš„ç½‘ç»œ è¾“å‡ºåœ°å€ [samples]
            reconstructDIR:     é‡æ„ç½‘ç»œçš„å­˜æ”¾åœ°å€ [reconstruction]
            link_possibility:   é‡æ„ç½‘ç»œæ—¶æŒ‡å®šçš„ è¿æ¥æƒé‡
        """
        # GAN å‚æ•°åˆå§‹åŒ–
        self.sess                = sess
        self.dataset_name        = dataset_name
        self.epoch               = epoch
        self.learning_rate       = learning_rate
        self.Momentum            = Momentum
        self.batch_size          = batch_size
        self.generatorFilter     = generatorFilter
        self.discriminatorFilter = discriminatorFilter
        self.generatorFC         = generatorFC
        self.discriminatorFC     = discriminatorFC
        # input / output size åˆå§‹åŒ–
        self.trainable_data_size = trainable_data_size
        self.inputMat_H          = inputMat_H
        self.inputMat_W          = inputMat_W
        self.outputMat_H         = outputMat_H
        self.outputMat_W         = outputMat_W
        self.OutDegreeLength     = OutDegree_Length
        # ç”¨ä½œGeneratorçš„è¾“å…¥ï¼Œç”Ÿæˆ å½“å‰ç½‘ç»œ
        self.InitSampleLength    = InitGen_Length
        # æŒ‡å®š è·¯å¾„
        self.inputPartitionDIR   = inputPartitionDIR
        self.checkpointDIR       = checkpointDIR
        self.sampleDIR           = sampleDIR
        self.reconstructDIR      = reconstructDIR
        # ç”¨ä½œç”Ÿæˆ æ‹“æ‰‘ç»“æ„ çš„æ–¹å¼
        self.link_possibility    = link_possibility

        """
        å› ä¸ºåé¢å¤šä¸ªåœ°æ–¹éƒ½éœ€è¦ç”¨åˆ° batch_norm æ“ä½œï¼Œå› æ­¤åœ¨è¿™é‡Œäº‹å…ˆè¿›è¡Œå®šä¹‰ --- based on DCGAN
        """
        self.generator_batch_norm_0 = batch_norm(name="g_bn0_%d"%self.inputMat_H)
        self.generator_batch_norm_1 = batch_norm(name="g_bn1_%d"%self.inputMat_H)
        self.generator_batch_norm_2 = batch_norm(name="g_bn2_%d"%self.inputMat_H)

        self.discriminator_batch_norm1 = batch_norm(name="d_bn1_%d"%self.inputMat_H)
        self.discriminator_batch_norm2 = batch_norm(name="d_bn2_%d"%self.inputMat_H)

        # æ„å»º GAN~
        self.modelConstrunction()

    def modelConstrunction(self):
        print('\n============================================================================')
        print('Model Construction ...')
        print('============================================================================')
        # 1. place holder è£…è¾“å…¥çš„æ•°æ®
        ## ç”¨ä½œå­˜å‚¨ çœŸå®æ•°æ®
        self.OutDegreeVector = tf.placeholder(tf.float32, shape=[self.batch_size, self.OutDegreeLength], name="Out_Degree_Vector_%d"%self.inputMat_H)
        self.inputMat = tf.placeholder(tf.float32, shape=[self.batch_size, self.inputMat_H, self.inputMat_W, 1], name="Real_Input_Adj_Matrix_%d"%self.inputMat_H)

        ## ç”¨ä½œå­˜å‚¨ éšæœºé‡‡æ ·çš„å±äºï¼Œç”¨äºGeneratorç”Ÿæˆå™¨çš„è¾“å…¥
        self.InitSampleVector = tf.placeholder(tf.float32, shape=[None, self.InitSampleLength], name="GEN_Input_%d"%self.inputMat_H)

        # 2. Generator
        self.Generator = self.__generator(InitInputSampleVector=self.InitSampleVector, Input_OutDegreeVector = self.OutDegreeVector, trainFlag=True)

        # 3. Sampler --- ç”¨ä½œ ç”Ÿæˆå›¾ç‰‡çš„æ—¶å€™~~
        self.Sampler = self.__generator(InitInputSampleVector=self.InitSampleVector, Input_OutDegreeVector = self.OutDegreeVector, trainFlag=False)

        # 4. Reconstruction --- ç”¨äº é‡å»ºç½‘ç»œæ—¶
        self.re_Mat = tf.placeholder(tf.float32, shape=[1, self.inputMat_H, self.inputMat_W, 1], name="Reconstruct_Input_Adj_Matrix_%d"%self.inputMat_H)
        self.re_OutDegreeVector = tf.placeholder(tf.float32, shape=[1, self.OutDegreeLength], name="Reconstruct_Out_Degree_Vector_%d"%self.inputMat_H)
        self.re_Construction = self.__re_Construction(InitInputSampleVector=self.InitSampleVector, Input_OutDegreeVector= self.re_OutDegreeVector, trainFlag=False)

        # 4. Discriminator_real
        self.Discriminator_real, self.D_logits_real = self.__discriminator(
            InputAdjMatrix=self.inputMat,
            Input_OutDegreeVector = self.OutDegreeVector,
            reuse_variables = False
        )
        # 5. Discriminator_fake
        self.Discriminator_fake, self.D_logits_fake = self.__discriminator(
            InputAdjMatrix=self.Generator,
            Input_OutDegreeVector=self.OutDegreeVector,
            reuse_variables=True
        )

        # 6. Discriminator LOSS FUNCTION
        ## åŠªåŠ›è®©çœŸå®æ•°æ®å…¨è¢«è¯†åˆ«ä¸º æ­£æ ·æœ¬ --- âˆ´ ä¸ºtf.ones()
        self.d_loss_real = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_real,labels=tf.ones_like(self.Discriminator_real))
        )
        ## åŠªåŠ›è®©Genç”Ÿæˆçš„æ•°æ®å…¨è¢«è¯†åˆ« è´Ÿæ ·æœ¬ --- âˆ´ ä¸ºtf.zeros()
        self.d_loss_fake = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_fake,labels=tf.zeros_like(self.Discriminator_fake))
        )
        self.discriminator_loss = self.d_loss_real + self.d_loss_fake

        # 7. Generator LOSS FUNCTION
        """ æ ¹æ®å…¬å¼~~
        æ³¨æ„ ç”Ÿæˆå™¨çš„Loss Functionåº”è¯¥ç®—Discriminatorçš„LOGITå“ˆï¼ï¼
        """
        # å³åŠªåŠ›è®©Genç”Ÿæˆçš„æ•°æ®å‘ æ­£æ ·æœ¬ å‰è¿› ^_^ --- âˆ´ ä¸ºtf.ones()
        self.generator_loss = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.Discriminator_fake,labels=tf.ones_like(self.D_logits_fake))
        )

        # --> æ”¾å…¥TensorBoardä¸­
        self.InitSampleVector_sum = tf.summary.histogram(name="GEN Input_%d"%self.inputMat_H,values=self.InitSampleVector)
        self.Generator_sum = tf.summary.histogram(name="GEN Output_%d"%self.inputMat_H,values=self.Generator)
        self.Discriminator_real_sum = tf.summary.histogram(name="Discriminator Real Output_%d"%self.inputMat_H, values=self.Discriminator_real)
        self.Discriminator_fake_sum = tf.summary.histogram(name="Discriminator Fake Output_%d"%self.inputMat_H, values=self.Discriminator_fake)

        self.d_loss_real_sum = tf.summary.scalar(name="Discriminator Real Loss_%d"%self.inputMat_H, tensor=self.d_loss_real)
        self.d_loss_fake_sum = tf.summary.scalar(name="Discriminator Fake Loss_%d"%self.inputMat_H, tensor=self.d_loss_fake)
        self.discriminator_loss_sum = tf.summary.scalar(name="Discriminator Loss_%d"%self.inputMat_H, tensor=self.discriminator_loss)

        self.generator_loss_sum = tf.summary.scalar(name="Generator Loss_%d"%self.inputMat_H,tensor=self.generator_loss)

        # --> å­˜å‚¨æ‰€æœ‰æ•°æ® --- based on DCGAN
        t_vars = tf.trainable_variables()

        self.discriminator_vars = [var for var in t_vars if "d_" in var.name]
        self.generator_vars = [var for var in t_vars if "g_" in var.name]

        # å­˜å‚¨checkpointæ—¶ç”¨ --- based on DCGAN
        self.saver = tf.train.Saver()

        print('down ...')

    def train(self,returnFlag=False):
        # å¦‚æœéœ€è¦returnï¼Œåˆ™å°†returnFlag=True, æ­¤æ—¶ä¼šè¿”å›åšå®Œæ‰€æœ‰epochä¹‹åï¼Œæ ¹æ®æ¯ä¸€å—é‡æ–°æ‹¼æ¥èµ·çš„ç½‘ç»œ
        print('\n============================================================================')
        print('Train Begin...')
        print('============================================================================')
        # 1. å®šä¹‰ åˆ¤åˆ«å™¨ å’Œ ç”Ÿæˆå™¨çš„ Loss  Function ä¼˜åŒ–æ–¹æ³•
        d_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.Momentum)\
            .minimize(self.discriminator_loss, var_list=self.discriminator_vars)
        g_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.Momentum)\
            .minimize(self.generator_loss, var_list = self.generator_vars)

        # 2. init all variables~ --- tf
        tf.global_variables_initializer().run()

        # 3. record all variables~ -- based on DCGAN
        self.generator_related_sum = tf.summary.merge([ self.InitSampleVector_sum,
                                                        self.Discriminator_fake_sum,
                                                        self.Generator_sum,
                                                        self.d_loss_fake_sum,
                                                        self.generator_loss_sum
                                                        ])
        self.discriminator_related_sum = tf.summary.merge([ self.InitSampleVector_sum,
                                                            self.Discriminator_real_sum,
                                                            self.d_loss_real_sum,
                                                            self.discriminator_loss_sum
                                                            ])
        self.writer = tf.summary.FileWriter("./boards", self.sess.graph)

        # 4. å‡†å¤‡æŠ•æ”¾è®­ç»ƒæ•°æ®~
        ## è¯»å…¥ mat & Outdegree
        """
        å¦‚æœå†…å­˜ä¸å¤Ÿ~ æ‰€ä»¥åœ¨åé¢å†è¯»~
        è¿™é‡Œæ²¡æœ‰å¿…è¦ä¸€æ¬¡æ€§è¯»å…¥
        """
        # data_mat, data_degree = self.__load_AdjMatInfo(AdjMat_OutDegree_Dir = self.inputPartitionDIR,startPoint=0,endPoint=20000,MatSize=self.inputMat_H)

        # 5. è®°å½•checkpoint --- based on DCGAN
        could_load, checkpoint_counter = self.load(self.checkpointDIR)
        if could_load is True:
            counter = checkpoint_counter
            print(" [*] Load SUCCESS")
        else:
            print(" [!] Load failed...")

        # 6. è®­ç»ƒå¼€å§‹~
        counter = 0
        start_time = time.time() # è®°å½•å½“å‰å¼€å§‹æ—¶é—´~~
        for epoch in range(self.epoch):
            # step = data_mat.shape[0] // self.batch_size
            step  = self.trainable_data_size // self.batch_size

            for idx in range(0,step):
                ## 1. è¯»å…¥æ‰€æœ‰æ•°æ® --- ç”±äºä¸€æ¬¡è¯»å…¥æœ‰ç‚¹å¤§ã€‚ã€‚ã€‚è¿™è¾¹åˆ†æ‰¹è¯»å…¥ å›§~~
                # batch_data_mat          = data_mat[idx*self.batch_size:(idx+1)*self.batch_size]
                # batch_data_degree       = data_degree[idx*self.batch_size:(idx+1)*self.batch_size]
                batch_data_mat, batch_data_degree = self.__load_AdjMatInfo( AdjMat_OutDegree_Dir = self.inputPartitionDIR,
                                                                            startPoint=idx*self.batch_size,
                                                                            endPoint=(idx+1)*self.batch_size,
                                                                            MatSize=self.inputMat_H)
                ## 1. ç”Ÿæˆç”¨äºGençš„è¾“å…¥æ•°æ®
                batch_generator_input   = np.random.uniform(low=-1,high=1,size=[self.batch_size, self.InitSampleLength]).astype(np.float32)

                ## 2. Update Discriminator åˆ¤åˆ«å™¨
                _, summary_str = self.sess.run( [d_optimizer, self.discriminator_related_sum],
                                                feed_dict={
                                                    self.inputMat: batch_data_mat,
                                                    self.InitSampleVector : batch_generator_input,
                                                    self.OutDegreeVector : batch_data_degree
                                                })
                self.writer.add_summary(summary_str, counter)

                ## 3. Update Generator ç”Ÿæˆå™¨
                _, summary_str = self.sess.run( [g_optimizer, self.generator_related_sum],
                                                feed_dict={
                                                    self.InitSampleVector : batch_generator_input,
                                                    self.OutDegreeVector : batch_data_degree
                                                })
                self.writer.add_summary(summary_str, counter)

                ## 4. Run g_optimizer twice to make sure that d_loss does not go to zero -- based on DCGAN
                _, summary_str = self.sess.run( [g_optimizer, self.generator_related_sum],
                                                feed_dict={
                                                    self.InitSampleVector : batch_generator_input,
                                                    self.OutDegreeVector : batch_data_degree
                                                })
                self.writer.add_summary(summary_str, counter)

                ## 5. è®°å½•æ¯æ¬¡çš„loss --- based on DCGAN
                errD_fake = self.d_loss_fake.eval({
                    self.InitSampleVector : batch_generator_input,
                    self.OutDegreeVector : batch_data_degree
                })
                errD_real = self.d_loss_real.eval({
                    self.inputMat : batch_data_mat,
                    self.OutDegreeVector : batch_data_degree
                })
                errG = self.generator_loss.eval({
                    self.InitSampleVector : batch_generator_input,
                    self.OutDegreeVector : batch_data_degree
                })

                ## 6. è¾“å‡º
                ### 1. æ¯æ¬¡éƒ½è¾“å‡º Loss Value
                counter += 1
                print("Epoch: [%d] | [%d/%d] time: %4.4f, d_loss: %.8f, g_loss: %.8f"%(epoch, idx, step, time.time()-start_time, errD_fake+errD_real, errG))
                ### 2. æ¯500æ¬¡ç”Ÿæˆä¸€å¼ å›¾ç‰‡ã€‚ã€‚ã€‚å¹¶ä¿å­˜~
                if counter % 500 == 0:
                    ## ç”¨ä½œGeneratorè¾“å…¥ ---  éšæœºç”Ÿæˆä¸€ä¸ªå…·æœ‰Self.InitSampleLenghté•¿åº¦çš„å‘é‡ï¼Œåˆ©ç”¨GANç”Ÿæˆä¸€ä¸ª Adj-Mat
                    sample_input = np.random.uniform(low=-1, high=1, size=[self.batch_size, self.InitSampleLength])
                    if debugFlag is True:
                        print('sampled input for Generator shape: ', sample_input.shape) # (20, 28)
                    # sample_mat = data_mat[0:self.batch_size]
                    # sample_labels = data_degree[0:self.batch_size]
                    # æ•°æ®é‡‡ç”¨åˆ†æ®µè½½å…¥æ–¹å¼, è¯»å…¥çœŸå®æ•°æ®æ˜¯ä¸ºäº†è®¡ç®— Loss å€¼
                    sample_mat, sample_labels = self.__load_AdjMatInfo( AdjMat_OutDegree_Dir = self.inputPartitionDIR,
                                                                        startPoint=0,
                                                                        endPoint=self.batch_size,
                                                                        MatSize=self.inputMat_H)

                    samples, d_loss, g_loss = self.sess.run([self.Sampler, self.discriminator_loss, self.generator_loss],
                                                            feed_dict={
                                                                self.InitSampleVector : sample_input,
                                                                self.inputMat : sample_mat,
                                                                self.OutDegreeVector : sample_labels
                                                            })

                    sample_folder = os.path.join(self.sampleDIR, "%s_%d_%d_%.1f"%(self.dataset_name, self.inputMat_H,self.trainable_data_size, self.link_possibility))
                    sample_folder = os.path.join(sample_folder,'')
                    """example: sample_folder = samples/WS_test_28_20000_0.5"""
                    save_topology(  adj=samples,
                                    path=sample_folder, graph_name = 'train_%d_%d'%(epoch,counter),
                                    link_possibility = self.link_possibility)
                    print("[Sample] d_loss: %.8f, g_loss: %.8f"%(d_loss, g_loss))

                if counter % 500 == 0:
                    # checkpointDIR = self.checkpointDIR
                    self.save(self.checkpointDIR,counter)

    def saveModel(self):
        """
        å­˜å‚¨è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        save_path = os.path.join('trained_Model', "%s_%d_%d_%.1f"%(self.dataset_name, self.inputMat_H,self.trainable_data_size, self.link_possibility),'')
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        saver = tf.train.Saver()
        saver.save(self.sess, save_path)
        return save_path

    def reconstructMat(self, type="Hierarchy"):
        print('\n============================================================================')
        print('Re-Construction Graph...')
        print('============================================================================')

        # step.0 é‡æ–°æ„å»ºæ¯ä¸€å—
        each_part = {}
        for i in range(self.trainable_data_size):
            # for generator input~ å› ä¸ºæ˜¯ ä¸€ä¸ªä¸€ä¸ªæ¢å¤ï¼Œ æ‰€ä»¥ä¸‹é¢ sample_input çš„size ä¸­ç¬¬ä¸€ä¸ªä»£è¡¨batch_sizeçš„å…ƒç´ åº”è¯¥æ˜¯ 1
            sample_input = np.random.uniform(low=-1, high=1.0, size=[1, self.InitSampleLength])

            # è¯»å…¥æ¯ä¸€ä¸ªçœŸå®å—çš„æ•°æ®ï¼Œåªæ˜¯ä¸ºäº†è®¡ç®— Loss å€¼ã€‚å…¶å®:
            """
            1. åªéœ€è¦æ¥¼ä¸Š sample_input       å°±å¯ä»¥è®©Generatorç”Ÿæˆç›¸åº”çš„ Adj_Mat~
            2. å†åŠ ä¸Šæ¥¼ä¸‹ partition_labels   å°±å¯ä»¥è®©Generatorç”ŸæˆæŸä¸€ç±»çš„ Adj_Mat~
            """
            partition_mat,partition_labels = self.__load_AdjMatInfo(AdjMat_OutDegree_Dir = self.inputPartitionDIR,
                                                                    startPoint=i,
                                                                    endPoint=i+1,
                                                                    MatSize=self.inputMat_H)

            #reconstruct_adj, d_loss, g_loss = self.sess.run([self.re_Construction, self.discriminator_loss, self.generator_loss],
                                                            #feed_dict={
                                                                #self.InitSampleVector : sample_input,
                                                                #self.re_Mat : partition_mat,
                                                                #self.re_OutDegreeVector : partition_labels
                                                            #})
            reconstruct_adj = self.sess.run([self.re_Construction],
                                            feed_dict={
                                                self.InitSampleVector : sample_input,
                                                self.re_Mat : partition_mat,
                                                self.re_OutDegreeVector : partition_labels
                                            })

            each_part[i] = np.squeeze(reconstruct_adj[0])

        # step.1  é‡‡ç”¨ é‡æ„å‡ºçš„reconstruct_adj ä»¥åŠ æ¯ä¸€å—çš„æ˜ å°„æ–‡ä»¶ <filename>_MatSize.map (part2Node) æ‹¼æ¥æ‰€æœ‰å—
        if type == "Hierarchy":
            reconstruct_folder = os.path.join(self.reconstructDIR, self.dataset_name, "Hierarchy",'')
            if not os.path.exists(reconstruct_folder):
                os.makedirs(reconstruct_folder)

            graph_name ="%s_%d_%d"%(self.dataset_name, self.inputMat_H,self.trainable_data_size)
            path = os.path.join('data',self.dataset_name,'')
            Node2Part = pickle.load(open(glob.glob(path+"*_%d.map"%self.inputMat_H)[0],'rb'))

            weighted_graph = construct_topology(graph_name, each_part, Node2Part)
            pickle.dump(weighted_graph, open(reconstruct_folder+graph_name+".nxgraph", 'wb'))


        elif type == "Condition":
            # under construction ... ğŸ˜¶
            pass

    def __generator(self, InitInputSampleVector, Input_OutDegreeVector, trainFlag=True):
        """
        @porpose å®ç° ç”Ÿæˆå™¨
        @ç”Ÿæˆå™¨æ¶æ„: ---> è¯¥ç”Ÿæˆå™¨ æ˜¯ä¸€ä¸ª åå·ç§¯ç½‘ç»œ
                RandomValueVector+OutDegreeVector -> generatorFC -> batch_norm_0 -> relu
                        -> generatorFC -> batch_norm_1 -> relu
                            -> batch_norm -> deconv -> batch_norm_2 -> relu
                                -> deconv -> batch_norm3 -> sigmoid -> generated_Adj-Matrix

        @InitInputSampleVector : æ”¾å…¥ ç”Ÿæˆå™¨ ä¸­çš„ ä¸€ä¸ª éšæœºçš„ åˆå§‹ å‘é‡
        @Input_OutDegreeVector : æ”¾å…¥ ç”Ÿæˆå™¨ ä¸­çš„ ä¸€ä¸ª Adj-Matrixçš„ å‡ºåº¦å…ƒç´ åˆ—è¡¨ å‘é‡
        @trainFlag             : æ˜¯å¦éœ€è¦è¿›è¡Œè®­ç»ƒ (å½“ä»…ä»…ä¸ºé‡‡æ ·æ—¶ä¸éœ€è¦è®­ç»ƒ)
        """
        with tf.variable_scope("generator") as scope:
            if trainFlag is False:
                scope.reuse_variables()

            # 1. å®šä¹‰ æ‰€æœ‰åå·ç§¯å±‚  å°ºå¯¸ --- å°ºå¯¸ä¸å·ç§¯å±‚çš„ä¿æŒä¸€è‡´
            Height, Width = self.outputMat_H, self.outputMat_W
            """p.s. ä¸ºä¿è¯/4èƒ½å¤Ÿé™¤å°½ï¼Œè¿™é‡Œå¼•å…¥ math.ceilå¯¹ç»“æœè¿›è¡Œå‘ä¸Šå–æ•´"""
            deconv1_Height, deconv1_Width = int(math.ceil(Height/2)), int(math.ceil(Width/2))
            deconv2_Height, deconv2_Weight = int(math.ceil(deconv1_Height/2)), int(math.ceil(deconv1_Width/2))

            #  2.  RandomValueVector+OutDegreeVector -> generatorFC -> batch_norm_0 -> relu := h0 -> h0+OutDegreeVector
            """p.s. ä¸‹é¢æ‰€æœ‰çš„æ‰€è°“"å‘é‡"å…¶å®éƒ½æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ç¬¬ä¸€ä¸ªç»´åº¦å°±æ˜¯batch_size"""
            ## 2.1 å°†éšæœº åˆå§‹çš„ å‘é‡ å’Œ å‡ºåº¦å…ƒç´ åˆ—è¡¨ å‘é‡ è¿›è¡Œæ‹¼æ¥
            input = tf.concat([InitInputSampleVector, Input_OutDegreeVector], axis=1) # <- input ä¸ºä¸€ä¸ª 28+28 å…± 56 çš„ å‘é‡
            if debugFlag is True:
                if trainFlag is True:
                    print('============================= generator =============================')
                elif trainFlag is False:
                    print('============================= sampler =============================')
                print('input shape: ',input.shape) # (20, 56)
            ## 2.2 ç»è¿‡å…¨è¿æ¥å±‚->è¿›è¡Œ å‘é‡åŒ–->é€šè¿‡æ¿€æ´»å‡½æ•°relu
            h0=tf.nn.relu(
                self.generator_batch_norm_0(
                    linear(input_=input, output_size=self.generatorFC, scope="g_h0_lin_%d"%self.inputMat_H),
                    train = trainFlag
                )
                ) # <- h0 ä¸ºä¸€ä¸ª 1024ç»´ çš„ å‘é‡
            if debugFlag is True:
                print('h0 shape: ', h0.shape) # (20, 1024)
            ## 2.3 å°†Input_OutDegreeVectoræ”¾åœ¨åé¢
            h0 = tf.concat([h0, Input_OutDegreeVector], axis=1) # <- æ·»åŠ h0 ä¸ºä¸€ä¸ª 1024ç»´ çš„å‘é‡ + 28 ç»´çš„é‚£ä¸ªOutDegreeVector [æ­¤æ—¶å°±å˜æˆäº† 20 * 1052 çš„çŸ©é˜µ]
            if debugFlag is True:
                print('h0+outdegree shape: ', h0.shape) # (20, 1052)

            # 3. h0+OutDegreeVector -> generatorFC -> batch_norm_1 -> relu := h1 -> reshape(h1) -> h1+OutDegreeVector
            output_size = self.generatorFilter*2 * deconv2_Height * deconv2_Weight  # <- è¿›ä¸€æ­¥å¢å¤§ç»´åº¦ï¼Œæ­¤æ—¶çš„ç»´åº¦åº”è¯¥æ˜¯åŸâ€œç¬¬äºŒæ¬¡å·ç§¯â€ä¹‹åçš„ç»´åº¦ã€‚
                                                                                    #  --- self.generatorFilter*2 è¡¨ç¤ºCNNç½‘ç»œæ‰€æœ‰çš„å·ç§¯filterå¤šå°‘
                                                                                    #  --- deconv2_Height * deconv2_Width è¡¨ç¤ºCNNç½‘ç»œç¬¬äºŒæ¬¡å·ç§¯ä¹‹åä¸€å…±çš„ä¸ªæ•°
                                                                                    #  åœ¨è¿™é‡Œï¼Œæœ‰ deconv2_Height = deconv2_Width = 28/4 = 7
                                                                                    #           self.generatorFilter*2 = 50*2 = 100
                                                                                    #           è¡¨æ˜ï¼Œçº¿æ€§åŒ–çš„é•¿åº¦åº”è¯¥æ˜¯ output_size = 100*7*7 = 4900
                                                                                    # âˆ´ h1.shape = [20, 4900]
            h1 = tf.nn.relu(
                self.generator_batch_norm_1(
                    linear(input_=h0, output_size=output_size, scope="g_h1_lin_%d"%self.inputMat_H),
                    train = trainFlag
                )
            )
            if debugFlag is True:
                print('h1 shape: ', h1.shape) # (20, 4900)
            # Â ç”±äºæ­¤æ—¶h1ä¸ºä¸€ä¸ª å‘é‡ï¼Œæ‰€ä»¥ä¸‹é¢å°†è¿™ä¸ª å‘é‡reshapeæˆä¸€ä¸ªTensor
            h1 = tf.reshape(h1, [self.batch_size, deconv2_Height, deconv2_Weight, self.generatorFilter*2])
            if debugFlag is True:
                print('reshape h1: ', h1.shape) # (20, 7,7, 100)
            # æ­¤æ—¶å°†reshapeå¥½çš„h1ä¸ä¹‹å‰çš„OutDegreeVectorè¿›è¡Œè¿½åŠ ã€‚
            # è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºä¹‹å‰OutDegreeVectorä»…ä»…æ˜¯ä¸€ä¸ª 20*28 çš„ å‘é‡ï¼Œè€Œè¿™é‡Œ h1ä»£è¡¨çš„æ˜¯ä¸€ä¸ªTensorï¼Œæ‰€ä»¥åº”è¯¥å…ˆå°†OutDegreeVectoræ‰©å±•æˆä¸€ä¸ªTensorï¼Œå†è¿›è¡Œè¿½åŠ 
            # ç”±äºh1.shape = [20, 7, 7, 100], æ‰€ä»¥å¯¹åº”çš„OutDegreeTensoråº”è¯¥æ˜¯æ¯ä¸€ä¸ªéƒ½æœ‰~æ‰€ä»¥åº”è¯¥å†™æˆä¸‹é¢è¿™ä¸ªå½¢å¼
            OutDegreeTensor = tf.reshape(self.OutDegreeVector, shape=[self.batch_size, 1, 1, self.OutDegreeLength])
            if debugFlag is True:
                print('reshape OutDegreeVector: ', OutDegreeTensor.shape) # (20, 1, 1, 28)
            # å°†OutDegreeTensoræ”¾åœ¨åé¢
            h1 = conv_cond_concat(x=h1, y=OutDegreeTensor)
            if debugFlag is True:
                print('reshape h1 + OutDegreeVector: ', h1.shape) # (20, 7, 7, 100+28)

            # 4. h1+OutDegreeVector -> deconv2d -> batch_norm_2 -> relu := h2 -> h2+OutDegreeVector
            h2 = tf.nn.relu(
                self.generator_batch_norm_2(
                    deconv2d(input_=h1, output_shape=[self.batch_size, deconv1_Height, deconv1_Width, self.generatorFilter*2], name="g_h2_%d"%self.inputMat_H),
                    train = trainFlag
                )
            )
            if debugFlag is True:
                print('deconv h2: ', h2.shape) # (20, 14, 14, 100)
            # å°†OutDegreeTensoræ”¾åœ¨åé¢
            h2 = conv_cond_concat(x=h2, y=OutDegreeTensor)
            if debugFlag is True:
                print('h2 + OutDegreeVector: ', h2.shape) # (20, 14, 14, 128)

            # 5. h2+OutDegreeVector -> deconv2d -> sigmoid # æ³¨æ„æœ€åä¸€å±‚æ²¡æœ‰batch_norm!!!
            h3 = tf.nn.sigmoid(
                deconv2d(input_=h2, output_shape=[self.batch_size, Height, Width, 1], name="g_h3_%d"%self.inputMat_H)
            )
            if debugFlag is True:
                print('final layer h3 shape: ', h3.shape) # (20, 28, 28, 1)

            return h3

    def __discriminator(self, InputAdjMatrix, Input_OutDegreeVector, reuse_variables=False):
        """
        @porpose å®ç° å¸¦æ¡ä»¶çš„ åˆ¤åˆ«å™¨ (å…¶ä¸­, â€œæ¡ä»¶â€æ˜¯æŒ‡ æ¯æ¬¡å·ç§¯çš„æ—¶å€™éœ€è¦å¸¦ä¸Šè¯¥Matrixçš„OutDegreeVectoræ¡ä»¶ï¼Œè¿™æ ·åšçš„æœŸæœ›æ˜¯ç”Ÿæˆ åŸºäºè¯¥OutDegreeVectorçš„Matrix)
        @ç”Ÿæˆå™¨æ¶æ„: ---> è¯¥åˆ¤åˆ«å™¨ æ˜¯ä¸€ä¸ª å·ç§¯ç½‘ç»œï¼Œå…¶ä¸­ï¼Œé‡‡ç”¨strideæ¥ä»£æ›¿ Max Pooling~  --- based on DCGAN
                    InputAdjMatrix+outDegreeVector -> condition(OutDegreeVector) & conv2d -> leaky_relu
                        -> condition(OutDegreeVector) & conv2d -> batch_norm_1 -> leaky_relu -> linear
                            -> FC -> batch_norm_2 -> leaky_relu
                                -> map_to_One_value -> sigmoid

        @InputAdjMatrix         : æ”¾å…¥ åˆ¤åˆ«å™¨ ä¸­çš„ ä¸€ä¸ª é‚»æ¥çŸ©é˜µ
        @Input_OutDegreeVector  : æ”¾å…¥ åˆ¤åˆ«å™¨ ä¸­çš„ ä¸€ä¸ª Adj-Matrixçš„å‡ºåº¦å…ƒç´ åˆ—è¡¨ å‘é‡
        @reuse_variables        : åˆ¤åˆ«å™¨æ˜¯å¦éœ€è¦å¤ç”¨æ•°æ®~ è¿™æ˜¯å› ä¸ºDä¼šæœ‰ä¸¤ä¸ªï¼Œè€Œè¿™ä¸¤ä¸ªå…¶å®éƒ½æŒ‡å‘çš„æ˜¯ä¸€ä¸ªD~ ---TF
        """
        with tf.variable_scope("discriminator") as scope:
            if reuse_variables is True:
                scope.reuse_variables() # å¦‚æœéœ€è¦reuseï¼Œé‚£ä¹ˆä¸‹é¢çš„æ‰€æœ‰nameéƒ½ä¼šåœ¨TFä¸­è¢«å¤æ–°ä½¿ç”¨~~

            if debugFlag is True:
                print('============================= discriminator =============================')
                print('input shape: ',InputAdjMatrix.shape) # (20, 28, 28, 1)
            # 1. é¦–å…ˆå°†OutDegreeVector è½¬æˆTensor å¹¶äºè¾“å…¥é‚»æ¥çŸ©é˜µæ‹¼æ¥åœ¨ä¸€èµ·
            OutDegreeTensor = tf.reshape(tensor=self.OutDegreeVector, shape=[self.batch_size, 1, 1, self.OutDegreeLength])
            if debugFlag is True:
                print('outdegree tensor: ', OutDegreeTensor.shape) # (20, 1,1, 28)
            input_ = conv_cond_concat(x=InputAdjMatrix, y=OutDegreeTensor) # (20, 28,28, 1+28)
            if debugFlag is True:
                print('input+outdegree vector shape: ',input_.shape) # (20, 28,28, 1+28)

            """ æ³¨æ„: è¿™é‡Œæ˜¯æ¡ä»¶çš„å·ç§¯~
                æ‰€ä»¥output_filterä¸€å®šæ·»åŠ ä¸Šæœ€åçš„OutDegreeçš„ç»“æœ~
            """
            # 2. ç»è¿‡ç¬¬ä¸€ä¸ªHidden_Layer: InputAdjMatrix+outDegreeVector -> condition(OutDegreeVector) & conv2d -> leaky_relu := h0 -> h0+outDegreeVector
            h0 = lrelu(conv2d(input_=input_, output_filter=1+self.OutDegreeLength, name="d_h0_conv_%d"%self.inputMat_H)) # æ¡ä»¶å·ç§¯!!!
            if debugFlag is True:
                print('CONDITION h0 shape: ', h0.shape) # (20, 14,14, 29)
            h0 = conv_cond_concat(x=h0,y=OutDegreeTensor)
            if debugFlag is True:
                print('h0+OutDegreeVector: ', h0.shape) # (20, 14,14, 29+28=57)

            # 3. ç»è¿‡ç¬¬äºŒä¸ªHidden_Layer: h0+outDegreeVector -> condition(OutDegreeVector) & conv2d -> batch_norm_1 -> leaky_relu -> linear := h1
            h1 = lrelu(self.discriminator_batch_norm1(
                conv2d(input_=h0, output_filter = self.discriminatorFilter + self.OutDegreeLength, name="d_h1_conv1_%d"%self.inputMat_H)
                                            )
                        )
            if debugFlag is True:
                print('CONDITION h1 shape: ', h1.shape) # (20, 7,7, 50+28)
            ## æ‰å¹³åŒ–, ä»¥é€‚ç”¨äºåé¢çš„å…¨è¿æ¥
            h1 = tf.reshape(tensor=h1, shape=[self.batch_size, -1])
            if debugFlag is True:
                print('linear h1: ', h1.shape) # (20, 7*7*78=3822)
            ## å†åŠ ä¸ŠOutDegreeVector(æ³¨æ„, æ­¤æ—¶ç”±äºæ‰å¹³åŒ–äº†ï¼Œæ‰€ä»¥åº”è¯¥é‡‡ç”¨outdegreeVector~~è€ŒéoutDegreeTensor)
            h1 = tf.concat(values=[h1,self.OutDegreeVector],axis=1)
            if debugFlag is True:
                print('h1+OutDegreeVector: ', h1.shape) # (20, 3822+28=3850)

            # 4. ç»è¿‡å…¨è¿æ¥å±‚: h1 -> FC -> batch_norm_2 -> leaky_relu := h2
            h2 = lrelu(self.discriminator_batch_norm2(linear(input_=h1, output_size=self.discriminatorFC, scope="d_h2_lin_%d"%self.inputMat_H)))
            if debugFlag is True:
                print('FC h2 shape: ', h2.shape) # (20, 1024)
            ## å†åŠ ä¸ŠOutDegreeVector
            h2 = tf.concat(values=[h2,self.OutDegreeVector], axis=1) # (20, 1024+28=1052)
            if debugFlag is True:
                print('h2 + OutDegreeVector: ', h2.shape) # (20, 1052)

            # 5. è½¬æ¢æˆ çœŸ/å‡, å¹¶é‡‡ç”¨sigmoidå‡½æ•°æ˜ å°„åˆ°[0,1]ä¸Š : h3 -> map_to_One_value -> sigmoid
            h3 = linear(input_=h2, output_size=1, scope="d_h3_lin_%d"%self.inputMat_H)
            if debugFlag is True:
                print('h3 map to ONE Value: ', h3.shape) # (20,1)
            h3_sigmoid = tf.nn.sigmoid(h3,name="sigmoid_h3_%d"%self.inputMat_H)
            if debugFlag is True:
                print('h3 to sigmoid: ', h3_sigmoid.shape) # (20, 1)

            return h3_sigmoid, h3

    def __re_Construction(self, InitInputSampleVector, Input_OutDegreeVector, trainFlag=False):
        """
        @porpose å®ç° ç”Ÿæˆå™¨
        @ç”Ÿæˆå™¨æ¶æ„: ---> è¯¥ç”Ÿæˆå™¨ æ˜¯ä¸€ä¸ª åå·ç§¯ç½‘ç»œ
                RandomValueVector+OutDegreeVector -> generatorFC -> batch_norm_0 -> relu
                        -> generatorFC -> batch_norm_1 -> relu
                            -> batch_norm -> deconv -> batch_norm_2 -> relu
                                -> deconv -> batch_norm3 -> sigmoid -> generated_Adj-Matrix

        @InitInputSampleVector : æ”¾å…¥ ç”Ÿæˆå™¨ ä¸­çš„ ä¸€ä¸ª éšæœºçš„ åˆå§‹ å‘é‡
        @Input_OutDegreeVector : æ”¾å…¥ ç”Ÿæˆå™¨ ä¸­çš„ ä¸€ä¸ª Adj-Matrixçš„ å‡ºåº¦å…ƒç´ åˆ—è¡¨ å‘é‡
        @trainFlag             : æ˜¯å¦éœ€è¦è¿›è¡Œè®­ç»ƒ (å½“ä»…ä»…ä¸ºé‡‡æ ·æ—¶ä¸éœ€è¦è®­ç»ƒ)
        """
        with tf.variable_scope("generator") as scope:
            if trainFlag is False:
                scope.reuse_variables()

            # 1. å®šä¹‰ æ‰€æœ‰åå·ç§¯å±‚  å°ºå¯¸ --- å°ºå¯¸ä¸å·ç§¯å±‚çš„ä¿æŒä¸€è‡´
            Height, Width = self.outputMat_H, self.outputMat_W
            """p.s. ä¸ºä¿è¯/4èƒ½å¤Ÿé™¤å°½ï¼Œè¿™é‡Œå¼•å…¥ math.ceilå¯¹ç»“æœè¿›è¡Œå‘ä¸Šå–æ•´"""
            deconv1_Height, deconv1_Width = int(math.ceil(Height/2)), int(math.ceil(Width/2))
            deconv2_Height, deconv2_Weight = int(math.ceil(deconv1_Height/2)), int(math.ceil(deconv1_Width/2))

            #  2.  RandomValueVector+OutDegreeVector -> generatorFC -> batch_norm_0 -> relu := h0 -> h0+OutDegreeVector
            """p.s. ä¸‹é¢æ‰€æœ‰çš„æ‰€è°“"å‘é‡"å…¶å®éƒ½æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ç¬¬ä¸€ä¸ªç»´åº¦å°±æ˜¯batch_size"""
            ## 2.1 å°†éšæœº åˆå§‹çš„ å‘é‡ å’Œ å‡ºåº¦å…ƒç´ åˆ—è¡¨ å‘é‡ è¿›è¡Œæ‹¼æ¥
            input = tf.concat([InitInputSampleVector, Input_OutDegreeVector], axis=1) # <- input ä¸ºä¸€ä¸ª 28+28 å…± 56 çš„ å‘é‡
            if debugFlag is True:
                if trainFlag is True:
                    print('============================= generator =============================')
                elif trainFlag is False:
                    print('============================= ReConstruction =============================')
                print('input shape: ',input.shape) # (20, 56)
            ## 2.2 ç»è¿‡å…¨è¿æ¥å±‚->è¿›è¡Œ å‘é‡åŒ–->é€šè¿‡æ¿€æ´»å‡½æ•°relu
            h0=tf.nn.relu(
                self.generator_batch_norm_0(
                    linear(input_=input, output_size=self.generatorFC, scope="g_h0_lin_%d"%self.inputMat_H),
                    train = trainFlag
                )
                ) # <- h0 ä¸ºä¸€ä¸ª 1024ç»´ çš„ å‘é‡
            if debugFlag is True:
                print('h0 shape: ', h0.shape) # (20, 1024)
            ## 2.3 å°†Input_OutDegreeVectoræ”¾åœ¨åé¢
            h0 = tf.concat([h0, Input_OutDegreeVector], axis=1) # <- æ·»åŠ h0 ä¸ºä¸€ä¸ª 1024ç»´ çš„å‘é‡ + 28 ç»´çš„é‚£ä¸ªOutDegreeVector [æ­¤æ—¶å°±å˜æˆäº† 20 * 1052 çš„çŸ©é˜µ]
            if debugFlag is True:
                print('h0+outdegree shape: ', h0.shape) # (20, 1052)

            # 3. h0+OutDegreeVector -> generatorFC -> batch_norm_1 -> relu := h1 -> reshape(h1) -> h1+OutDegreeVector
            output_size = self.generatorFilter*2 * deconv2_Height * deconv2_Weight  # <- è¿›ä¸€æ­¥å¢å¤§ç»´åº¦ï¼Œæ­¤æ—¶çš„ç»´åº¦åº”è¯¥æ˜¯åŸâ€œç¬¬äºŒæ¬¡å·ç§¯â€ä¹‹åçš„ç»´åº¦ã€‚
                                                                                    #  --- self.generatorFilter*2 è¡¨ç¤ºCNNç½‘ç»œæ‰€æœ‰çš„å·ç§¯filterå¤šå°‘
                                                                                    #  --- deconv2_Height * deconv2_Width è¡¨ç¤ºCNNç½‘ç»œç¬¬äºŒæ¬¡å·ç§¯ä¹‹åä¸€å…±çš„ä¸ªæ•°
                                                                                    #  åœ¨è¿™é‡Œï¼Œæœ‰ deconv2_Height = deconv2_Width = 28/4 = 7
                                                                                    #           self.generatorFilter*2 = 50*2 = 100
                                                                                    #           è¡¨æ˜ï¼Œçº¿æ€§åŒ–çš„é•¿åº¦åº”è¯¥æ˜¯ output_size = 100*7*7 = 4900
                                                                                    # âˆ´ h1.shape = [20, 4900]
            h1 = tf.nn.relu(
                self.generator_batch_norm_1(
                    linear(input_=h0, output_size=output_size, scope="g_h1_lin_%d"%self.inputMat_H),
                    train = trainFlag
                )
            )
            if debugFlag is True:
                print('h1 shape: ', h1.shape) # (20, 4900)
            # Â ç”±äºæ­¤æ—¶h1ä¸ºä¸€ä¸ª å‘é‡ï¼Œæ‰€ä»¥ä¸‹é¢å°†è¿™ä¸ª å‘é‡reshapeæˆä¸€ä¸ªTensor
            h1 = tf.reshape(h1, [1, deconv2_Height, deconv2_Weight, self.generatorFilter*2])
            if debugFlag is True:
                print('reshape h1: ', h1.shape) # (20, 7,7, 100)
            # æ­¤æ—¶å°†reshapeå¥½çš„h1ä¸ä¹‹å‰çš„OutDegreeVectorè¿›è¡Œè¿½åŠ ã€‚
            # è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºä¹‹å‰OutDegreeVectorä»…ä»…æ˜¯ä¸€ä¸ª 20*28 çš„ å‘é‡ï¼Œè€Œè¿™é‡Œ h1ä»£è¡¨çš„æ˜¯ä¸€ä¸ªTensorï¼Œæ‰€ä»¥åº”è¯¥å…ˆå°†OutDegreeVectoræ‰©å±•æˆä¸€ä¸ªTensorï¼Œå†è¿›è¡Œè¿½åŠ 
            # ç”±äºh1.shape = [20, 7, 7, 100], æ‰€ä»¥å¯¹åº”çš„OutDegreeTensoråº”è¯¥æ˜¯æ¯ä¸€ä¸ªéƒ½æœ‰~æ‰€ä»¥åº”è¯¥å†™æˆä¸‹é¢è¿™ä¸ªå½¢å¼
            OutDegreeTensor = tf.reshape(self.re_OutDegreeVector, shape=[1, 1, 1, self.OutDegreeLength])
            if debugFlag is True:
                print('reshape OutDegreeVector: ', OutDegreeTensor.shape) # (20, 1, 1, 28)
            # å°†OutDegreeTensoræ”¾åœ¨åé¢
            h1 = conv_cond_concat(x=h1, y=OutDegreeTensor)
            if debugFlag is True:
                print('reshape h1 + OutDegreeVector: ', h1.shape) # (20, 7, 7, 100+28)

            # 4. h1+OutDegreeVector -> deconv2d -> batch_norm_2 -> relu := h2 -> h2+OutDegreeVector
            h2 = tf.nn.relu(
                self.generator_batch_norm_2(
                    deconv2d(input_=h1, output_shape=[1, deconv1_Height, deconv1_Width, self.generatorFilter*2], name="g_h2_%d"%self.inputMat_H),
                    train = trainFlag
                )
            )
            if debugFlag is True:
                print('deconv h2: ', h2.shape) # (20, 14, 14, 100)
            # å°†OutDegreeTensoræ”¾åœ¨åé¢
            h2 = conv_cond_concat(x=h2, y=OutDegreeTensor)
            if debugFlag is True:
                print('h2 + OutDegreeVector: ', h2.shape) # (20, 14, 14, 128)

            # 5. h2+OutDegreeVector -> deconv2d -> sigmoid # æ³¨æ„æœ€åä¸€å±‚æ²¡æœ‰batch_norm!!!
            h3 = tf.nn.sigmoid(
                deconv2d(input_=h2, output_shape=[1, Height, Width, 1], name="g_h3_%d"%self.inputMat_H)
            )
            if debugFlag is True:
                print('final layer h3 shape: ', h3.shape) # (20, 28, 28, 1)

            return h3

    def __load_AdjMatInfo(self, AdjMat_OutDegree_Dir, startPoint=0, endPoint=50, MatSize=-1):
        if MatSize == -1:
            sys.exit('[!!!] Please specify mat size... otherwise the program cannot understand which partition results is needed.')
        path = os.path.join('data',AdjMat_OutDegree_Dir)
        path = os.path.join(path, '')
        if debugFlag is True:
            print('current data file path: ', path)
        adj_file = glob.glob(path+'*_%d.graph'%MatSize)[0]
        if debugFlag is True:
            print('current adj file dir: ', adj_file, end='\t')
        degree_file = glob.glob(path+'*_%d.degree'%MatSize)[0]

        # è¯»å…¥adj
        adj = pickle.load(open(adj_file,'rb'))
        degree = pickle.load(open(degree_file,'rb'))
        if debugFlag is True:
            print('loaded adj file, adj.keys() = ', len(adj.keys()), end='\t')
            print('loaded degree file, degree.keys() = ', len(degree.keys()), end='\t')
            print('start point= ', startPoint, '\tend point= ', endPoint)
        # å°†adjå­—å…¸è½¬æ¢æˆTensor
        for i in range(startPoint, endPoint):
            # if debugFlag is True:
            #     if i+1 % 5000 == 0:
            #         print('transfer %d Tensors'%i)
            if i == startPoint:
                if i not in adj.keys():
                    i = len(adj.keys())-1
                #Tensor = tf.stack([adj[i]],axis=0)
                Tensor = np.stack([adj[i]],axis=0)
                #DegreeTensor = tf.stack([degree[i]],axis=0)
                DegreeTensor = np.stack([degree[i]],axis=0)
            else:
                if i not in adj.keys():
                    i = len(adj.keys())-1
                #slice = tf.stack([adj[i]],axis=0)
                slice = np.stack([adj[i]],axis=0)
                #slice_degree = tf.stack([degree[i]],axis=0)
                slice_degree = np.stack([degree[i]],axis=0)
                #Tensor = tf.concat([Tensor, slice],axis=0)
                Tensor = np.concatenate([Tensor, slice],axis=0)
                #DegreeTensor = tf.concat([DegreeTensor, slice_degree],axis=0)
                DegreeTensor = np.concatenate([DegreeTensor, slice_degree],axis=0)

        #AdjMatTensor = tf.expand_dims(input=Tensor, axis=-1)
        AdjMatTensor = np.expand_dims(Tensor, axis=-1)
        if debugFlag is True:
            print('Output -> AdjMat Tensor shape: ', AdjMatTensor.shape, end='\t')
            print('Output -> Degree Tensor shape: ', DegreeTensor.shape)

        #return AdjMatTensor.eval(),DegreeTensor.eval()
        return AdjMatTensor,DegreeTensor
    # ================================================
    # FROM DCGAN
    @property
    def model_dir(self):
        return "{}_{}_{}_{}".format(self.dataset_name, self.inputMat_H, self.trainable_data_size, self.link_possibility)

    def save(self, checkpoint_dir, step):
        # model_name = "DCGAN.model"
        model_name = "AdjMatrixGenerator.model"
        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        self.saver.save(self.sess,os.path.join(checkpoint_dir, model_name),global_step=step)

    def load(self, checkpoint_dir):
        import re
        print(" [*] Reading checkpoints...")
        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)

        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)
            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))
            counter = int(next(re.finditer("(\d+)(?!.*\d)",ckpt_name)).group(0))
            print(" [*] Success to read {}".format(ckpt_name))
            return True, counter
        else:
            print(" [*] Failed to find a checkpoint")
            return False, 0
    # From DCGAN END
    # ================================================



